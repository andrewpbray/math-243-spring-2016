---
title: "Regression Trees"
output:
  ioslides_presentation:
    incremental: true
---

## Ex: MLB {.vcenter .flexbox}

Can we predict the `Salary` of a MLB player based on the number of:

- `Years` that he has been in the league
- `Hits` that he made in the previous season

<img src="http://a.espncdn.com/photo/2011/0615/pg2_g_scmidt_sy_576.jpg"> 


##

```{r}
library(ISLR)
dim(Hitters)
names(Hitters)
```


## {.build}

```{r echo = FALSE}
library(ISLR)
data(Hitters)
library(scatterplot3d)
s3d <-scatterplot3d(Hitters$Years, Hitters$Hits, Hitters$Salary, pch=16, highlight.3d=TRUE,
  type="h", main="", xlab = "Years", ylab = "Hits", zlab = "Salary")
fit <- lm(Salary ~ Years + Hits, data = Hitters)
```

Looks like a good setting for . . . regression.

## We get: predictions {.build}

```{r echo = FALSE}
library(ISLR)
data(Hitters)
library(scatterplot3d)
s3d <-scatterplot3d(Hitters$Years, Hitters$Hits, Hitters$Salary, pch=16, highlight.3d=TRUE,
  type="h", main="", xlab = "Years", ylab = "Hits", zlab = "Salary")
fit <- lm(Salary ~ Years + Hits, data = Hitters)
s3d$plane3d(fit)
m1 <- lm(Salary ~ Years + Hits, data = Hitters)
```


## We get: predictions {.build}

$$
MSE_{train} = \frac{1}{n}RSS
$$

```{r}
mean(m1$res^2)
```

(Or better, use $CV_{(k)}$)


## We get: a probability model {.build}

OLS regression:

$$
P(Y = y \, | \, X = x) = N(\beta_0 + \beta_1x, \sigma^2)
$$

```{r echo = FALSE}
par(mfrow = c(1, 2))
plot(m1, 1)
plot(m1, 2)
```


## Quick fix?

```{r}
Hitters$logSalary <- log(Hitters$Salary)
m2 <- lm(logSalary ~ Years + Hits, data = Hitters)
```

```{r echo = FALSE}
par(mfrow = c(1, 2))
plot(m2, 1)
plot(m2, 2)
```


## We get: description {.build}

```{r}
summary(m2)$coef
```

- An *increase* in `Years` is associated with an *increase* in Salary, on average.
- An *increase* in `Hits` is associated with an *increase* in Salary, on average.

How can we figure out which one is more important?


# Regression Trees

## {.build .vcenter .flexbox}

**Regression Tree**: a method to predict a continuous response, $Y$, using a series
of $p$ predictors, $X$, by recursive binary splitting to minimize RSS.

<!-- Credit: Barry Pate -->
<img src="../figs/oak.jpeg">


## {.vcenter .flexbox}

**Regression Tree**: a method to predict a continuous response, $Y$, using a series
of $p$ predictors, $X$, by recursive binary splitting to minimize RSS.

<!-- Credit: Barry Pate -->
<img src="../figs/oakflip.jpg">


##

```{r echo = FALSE}
library(tree)
t1 <- tree(logSalary ~ Years + Hits, data = Hitters)
t2 <- prune.tree(t1, best = 3)
plot(t2)
text(t2, pretty = 0)
```


## Interpretation {.build}

- `Years` is the most important factor in contributing to salary, with less-experienced
players earning less.
- Given a player is less-experienced, `Hits` has little impact on `Salary`.
- Given a player is more experienced, those with more `Hits` have a higher `Salary`.


## 
**Practice #1**: Draw the predictor space corresponding to the following tree.

```{r echo = FALSE}
t3 <- tree(mpg ~ hp + wt, data = mtcars)
plot(t3)
text(t3, pretty = 0)
```

What would you expect the signs the regression slopes to be?

##

```{r echo = FALSE}
m2 <- lm(mpg ~ hp + wt, data = mtcars)
summary(m2)$coef
```


## Practice #2


## The Algorithm {.build}

1. Use RBS to grow a large tree on full data, stopping when every leaf has a 
small number of obs.
2. Apply cost-complexity pruning to obtain a best subtree for many values of $\alpha$.
3. Use k-fold CV to choose $\alpha$. For each fold:
    - Repeat (1) and (2) on training data.
    - Compute the test MSE on all subtrees (one test MSE per $\alpha$).
    Average the test MSEs for each $\alpha$ and choose $\alpha$ that minimizes.
4. Use that $\alpha$ to select your best subtree in (2)

# Classification Trees

## Ex. MLB

```{r}
names(Hitters)
```

Can we predict the league that a player is in based on his other variables?


```{r echo = FALSE}
library(tree)
t4 <- tree(League ~. - NewLeague, data = Hitters, split = "gini")
t5 <- prune.tree(t4, best = 4)
plot(t5)
text(t5, pretty = 0)
```

