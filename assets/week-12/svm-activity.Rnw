\documentclass{article}
\usepackage{graphicx, color, hyperref, fancyhdr}

%\input{../brayTeachingStyle}

\usepackage[top=.8in, bottom=.5in, left=1.5in, right=1.5in]{geometry}
\thispagestyle{empty}
\begin{document}

\begin{center}
\textsc{Math 243: Statistical Learning} \\
\noindent\rule{12cm}{.5pt}
\end{center}

\subsection*{Separating planes}

% for next time: start with a 1D example and ask for a formal rule. that will
% get them used to the formalization before the 2D example, and then you can return
% to the 1D non separable case when you discuss the expansion of the feature space.

The two scatterplots below represent two training data sets where there are two predictors, $X_1$ and $X_2$, and a two class response, $Y \in \{\textrm{red}, \textrm{blue} \}$. Use a straight edge to draw a decision boundary that you expect to minimize the testing misclassification rate. Below each image write down the formal geometric \emph{rule} that you used to construct a decision boundary between the two classes.

\vspace{5mm}

<<echo = FALSE, warning = FALSE, fig.height=4, fig.width = 8>>=
library(ggplot2)
d <- data.frame("X1" = scale(mtcars$hp),
                "X2" = scale(mtcars$disp),
                "group" = as.factor(mtcars$vs))
p2 <- ggplot(d, aes(x = X1, y = X2, col = group)) +
  geom_point(size = 2.5) +
  scale_colour_discrete(guide = FALSE) +
  theme_bw()

d$group[d$X2 < 0 ] <- 1 
p1 <- ggplot(d, aes(x = X1, y = X2, col = group)) +
  geom_point(size = 2) +
  scale_colour_discrete(guide = FALSE) +
  theme_bw()
library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
@

\vspace{5mm}

\textbf{Rule:} \hspace{65mm} \textbf{Rule:}

\vspace{70mm}

\noindent Now let's relax the constraint that the decision boundary is linear. For both plots above, draw a revised dashed line that can be curved if you think that would be benefitial. It's difficult to formalize a geometric rule for a hand-drawn curve, but what general principles did you use when deciding on your new boundaries?



\end{document}