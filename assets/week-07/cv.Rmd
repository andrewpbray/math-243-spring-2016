---
title: "Resampling Methods"
output:
  ioslides_presentation:
    incremental: true
---

## Resampling

Def: *Draw many samples from the training data and refit the model to each in order to learn about your model.*


# Cross-Validation

## Cross-Validation {.build}

Methods to estimate the test error rate (MSE, misclassification error) by

- holding out a subset of othe training data from the fitting process
- using that model to predict on the subset

### Three Flavors
1. Validation Set
2. Leave-one-out
3. $k$-fold


## Validation Set {.build}

1. Randomly split data into a *training set* and a *validation set*.
2. Fit model to *training set*.
3. Use model to predict responses for *validation set*.
4. Compute validation set error rate as estimate of test error rate.

Let's look back on how we compared Logistic Regression to LDA . . .


## Split data into training/validation

```{r}
library(ISLR)
data(Default)
set.seed(39)
test_ind <- sample(1:10000, size = 5000)
Default_test <- Default[test_ind, ]
Default_train <- Default[-test_ind, ]
```


## Fit models to train

```{r message = FALSE}
m1 <- glm(default ~ balance, data = Default_train, family = binomial)

library(dplyr)
est <- Default_train %>%
  group_by(default) %>%
  summarize(n = n(),
            prop = n/nrow(Default_train),
            mu = mean(balance),
            ssx = var(balance) * (n - 1))
```

## Fit models to train {.build}

Pull off estimates (and convert class).

```{r}
pi_n <- as.numeric(est[1, 3])
pi_y <- as.numeric(est[2, 3])
mu_n <- as.numeric(est[1, 4])
mu_y <- as.numeric(est[2, 4])
sig_sq <- (1/(nrow(Default_train) - 2)) * sum(est$ssx)
```




## Make predictions on validation

```{r}
log_pred <- predict(m1, newdata = Default_test, type = "response")

my_lda <- function(x, pi, mu, sig_sq) {
  x * (mu/sig_sq) - (mu^2)/(2 * sig_sq) + log(pi)
}
d_n <- my_lda(Default_test$balance, pi_n, mu_n, sig_sq)
d_y <- my_lda(Default_test$balance, pi_y, mu_y, sig_sq)

my_log_pred <- ifelse(log_pred < 0.5, "No", "Yes")
my_lda_pred <- ifelse(d_n > d_y, "No", "Yes")
```


## Misclassification Rates

```{r}
conf_log <- table(my_log_pred, Default_test$default)
conf_lda <- table(my_lda_pred, Default_test$default)
(1/nrow(Default_test)) * (conf_log[2, 1] + conf_log[1, 2])
(1/nrow(Default_test)) * (conf_lda[2, 1] + conf_lda[1, 2])
```

**LDA has the lower test error rate**.


## Let's do that again {.build}

How would have our conclusions changed if we had used a different partition into training and validation sets?

```{r}
test_ind <- sample(1:10000, size = 5000)
Default_test <- Default[test_ind, ]
Default_train <- Default[-test_ind, ]
```

(Re-run all that code)

```{r message = FALSE, echo = FALSE}
valid_demo <- function() {
m1 <- glm(default ~ balance, data = Default_train, family = binomial)

est <- Default_train %>%
  group_by(default) %>%
  summarize(n = n(),
            prop = n/nrow(Default_train),
            mu = mean(balance),
            ssx = var(balance) * (n - 1))

pi_n <- as.numeric(est[1, 3])
pi_y <- as.numeric(est[2, 3])
mu_n <- as.numeric(est[1, 4])
mu_y <- as.numeric(est[2, 4])
sig_sq <- (1/(nrow(Default_train) - 2)) * sum(est$ssx)

log_pred <- predict(m1, newdata = Default_test, type = "response")

my_lda <- function(x, pi, mu, sig_sq) {
  x * (mu/sig_sq) - (mu^2)/(2 * sig_sq) + log(pi)
}
d_n <- my_lda(Default_test$balance, pi_n, mu_n, sig_sq)
d_y <- my_lda(Default_test$balance, pi_y, mu_y, sig_sq)

my_log_pred <- ifelse(log_pred < 0.5, "No", "Yes")
my_lda_pred <- ifelse(d_n > d_y, "No", "Yes")

list(log = table(my_log_pred, Default_test$default),
     lda = table(my_lda_pred, Default_test$default))
}

o <- valid_demo()

conf_log <- o$log
conf_lda <- o$lda
```

```{r}
(1/nrow(Default_test)) * (conf_log[2, 1] + conf_log[1, 2])
(1/nrow(Default_test)) * (conf_lda[2, 1] + conf_lda[1, 2])
```


## Let's do that many times

```{r echo = FALSE, message=FALSE}
it <- 500
g <- data.frame(logo = rep(NA, 500), ldao = rep(NA, 500))
for(i in 1:it) {
  test_ind <- sample(1:10000, size = 5000)
  Default_test <- Default[test_ind, ]
  Default_train <- Default[-test_ind, ]
  o <- valid_demo()
  conf_log <- o$log
  conf_lda <- o$lda
  g[i, 1] <- (1/nrow(Default_test)) * (conf_log[2, 1] + conf_log[1, 2])
  g[i, 2] <- (1/nrow(Default_test)) * (conf_lda[2, 1] + conf_lda[1, 2])
}

library(ggplot2)
df <- data.frame(err = c(g[, 1], g[, 2]),
                 mod = as.factor(rep(c("log", "lda"), each = it)))
ggplot(df, aes(x = err, color = mod)) + geom_density()
```


## Validation Set

### Downsides

1. Estimate of test error rate can be highly variable based on the partition.
2. You only use a fraction of the data to fit the model > overestimating test error rate.


# Leave-one-out





