---
title: "Boosting"
output:
  ioslides_presentation:
    incremental: true
---
# Quiz Review

## The Algorithm {.build}

1. Use RBS to grow a large tree on full data, stopping when every leaf has a 
small number of obs.
2. Apply cost-complexity pruning to obtain a best subtree for many values of $\alpha$.
3. Use k-fold CV to choose $\alpha$. For each fold:
    - Repeat (1) and (2) on training data.
    - Compute the test MSE on all subtrees (one test MSE per $\alpha$).
    Average the test MSEs for each $\alpha$ and choose $\alpha$ that minimizes.
4. Use that $\alpha$ to select your best subtree in (2)


# Bagging and Random Forests

## Review {.build}

### Goal
Decrease the variance of our estimates.

### Strategies

**Bagging**: create many **b**oostrapped data sets, fit a full tree to each, then **agg**regate the predictions.

**Random Forests**: perform bagging, but for each tree, at each split, only consider $m \le p$ predictors.

$$ m = \sqrt(p) \quad \quad m = p/3$$


## Random Forests: tree 1 {.build}

Let's take a look at what goes into building the first random tree. We're working with the same MLB data set.

```{r echo = FALSE}
set.seed(94)
```

```{r message = FALSE}
library(ISLR)
data(Hitters)
```

The first step is to bootstrap a data set.

```{r}
boot_ind <- sample(1:nrow(Hitters), replace = TRUE)
hit_boot <- Hitters[boot_ind, ]
```


## A random candidate set {.build}

```{r}
m <- sqrt(ncol(hit_boot))
names(hit_boot)
rforest_ind <- sample(1:ncol(hit_boot), size = m, replace = FALSE)
rforest_ind
hit_rforest <- hit_boot[ , c(rforest_ind, 14)]
```


## The first random split {.build}

```{r message = FALSE, fig.height=15}
library(tree)
t1 <- tree(League ~ ., data = hit_rforest)
plot(t1)
text(t1, pretty = 0)
```


## Subsequent splits {.build}

- Each split takes a new random sample (without replacement) of $m$ predictors as candiates.
- This can be done more succinctly with `randomForest()` in `library(randomForest())`.


## Random Forests: tree 2 {.build}

Create a new bootstrap a data set.

```{r}
boot_ind <- sample(1:nrow(Hitters), replace = TRUE)
hit_boot <- Hitters[boot_ind, ]
```


## A random candidate set {.build}

```{r}
m <- sqrt(ncol(hit_boot))
names(hit_boot)
rforest_ind <- sample(1:ncol(hit_boot), size = m, replace = FALSE)
rforest_ind
hit_rforest <- hit_boot[ , c(rforest_ind, 14)]
```


## The first random split {.build}

```{r message = FALSE, fig.height=15}
library(tree)
t1 <- tree(League ~ ., data = hit_rforest)
plot(t1)
text(t1, pretty = 0)
```


## Making a final prediction {.build}

- Predictor values are plugged into each random tree to make a prediction.
- Final prediction is made by majority vote (classification) or averaging (regression).


# Boosting

## Consider the ant and the bee {.vcenter .flexbox .build}

<img src="../figs/ant-and-bee.png" alt="ant bee" height="400" width="600"> 

On his own, the bee is far too weak to move the bee ...


## Consider the ant and the bee {.vcenter .flexbox .build}

<img src="../figs/ants-carrying-bee.jpg" alt="ant bee" height="370" width="550"> 

... but combine many complementary weak components, and you get something that's very powerful.


## Boosting {.build}

A very general statistical technique that builds up a sequence of weak complementary learners into a boosted learner that is strong.

Note: there is no bootstrapping needed.


## Growing a weak tree {.build}

Back to the regression setting...

```{r}
t2 <- tree(Salary ~ ., data = Hitters)
```

```{r echo = FALSE}
plot(t2)
text(t2, pretty = 0)
```

## Growing a weak tree {.build}

Let's say we want each weak tree to contain only one split $d = 1$.

```{r}
weak1 <- prune.tree(t2, best = 2)
```

```{r echo = FALSE, fig.height=3}
plot(weak1)
text(weak1, pretty = 0)
```


## Finding a complement {.build .smaller}

How can we figure out where this weak learner failed to explain the data well?

**Residuals**: $r_i = y_i - \hat{y}_i$

```{r}
Hitters$Salary[5:8]
predict(weak1, newdata = Hitters)[5:8]
Hitters$Salary[5:8] - predict(weak1, newdata = Hitters)[5:8]
```


## Growing a new weak tree {.build}

```{r}
r1 <- Hitters
r1$Salary <- Hitters$Salary - predict(weak1, newdata = Hitters)
t3 <- tree(Salary ~ ., data = r1)
weak2 <- prune.tree(t3, best = 2)
```

```{r echo = FALSE, fig.height=3}
plot(weak2)
text(weak2, pretty = 0)
```


## And so on... {.builds}

- Continue process of fitting simple trees (weak learners) to the residuals from the previous model.
- Stop after you've made a total of $B$ trees.
- Final boosted prediction:

$$ \hat{f}(x) = \sum_{b = 1}^B \hat{f}^b(x)$$

well, that's close, but we can be a bit more flexible.





