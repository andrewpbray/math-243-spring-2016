\documentclass[11pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\author{Key (with contributions from E. Peairs and P. Stallworth)}
\usepackage{fullpage}
\title{Problem Set 4}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle
\begin{enumerate}
\item[4.]
\begin{enumerate}[label=(\alph*)]
\item Note carefully the language: \emph{within 10\% of the range of $X$ closest to that test observation}. That's a bit amgibuous. It's helpful that they give the example that suggests that the total range should be 10\%. So it's a fair interpretation to think that at $X = 0$ we'd have from $[0, 0.1]$ to use in making our prediction.

The fraction of observations available for prediction is proportional to the proportion of the range of $X$ utilizd by the model. Here, it will be 10\%. It's possible to interpret their language differently here, in which case you might come up with an answer somewhat lower than $10\%$.
\item Now we're using the squared percentage from last time, or $0.1^2 = 0.01 = 1\%$. This represents the proportion of the unit square covered by a square with a side of length 0.1.
\item Now we're using $0.1^100 = 1e-100$ of the data.
\item We can see from this that each time you add a predictor, you reduce the number of observations by a factor of the fractional window size, which gets really tiny for large $p$.
\item Since we normalized all the data to the range $[0,1]$, the volume of the data space is always 1. Thus, we always want to contain a volume of 0.1. If $l$ is the length of the cube's side, our requirement is that $l^p = 0.1$, so $l = 0.1^{1/p}$. This yields $l = 0.1, 0.316, \textrm{and } 0.978$ for $p = 1, 2, \textrm{and } 100$, respectively.
\end{enumerate}
\item[6.]
\begin{enumerate}[label=(\alph*)]
\item Plugging in the provided values to the logistic function, $p=\dfrac{1}{1+e^{6-0.05\cdot 40 - 1 \cdot 3.5}} = 0.38$
\item Varying $X_1$ and holding $X_2$ fixed, we need to solve
\[ 0.5 = \dfrac{1}{1+e^{6 - 0.05 \cdot X_1 - 3.5}} \]
\[ \implies 0.5 + 0.5 e^{2.5 - 0.05 \cdot X_1 } = 1 \]
\[ \implies e^{2.5 - 0.05 \cdot X_1 } = 1\]
\[ \implies 2.5 = 0.005 X_1\]
\[ \implies X_1 = 50\]
\end{enumerate}

\item[7.] \textbf{Mathematical approach}: We have that $\pi_{yes} = 0.8$, and also that $f_{yes}(x) = \frac{1}{\sqrt{2\pi \cdot 36}} e^{-(x-10)^2/2\cdot 36} = \frac{1}{\sqrt{72 \pi}} e^{-(x-10)^2/72}$. Plugging into Bayes' theorem, we can calculate our probability to be
\[ \dfrac{0.8\cdot  \frac{1}{\sqrt{72 \pi}} e^{-(x-10)^2/72}}    {0.8 \cdot \frac{1}{\sqrt{72 \pi}} e^{-(x-10)^2/72}+ 0.2 \cdot  \frac{1}{\sqrt{72 \pi}} e^{-(x)^2/72}} = \dfrac{1}{1 + \frac14 e^{(-20x + 100)/72}}\]

Plugging in $X = 4$ gives us 

\[p(4) = \dfrac{1}{1 + \dfrac14 e^{(-80 + 100)/72}} = 0.7518 \]

\textbf{Computational approach}: From the problem, we (approximately) learn $X|(Y = \text{No}) \sim N(0, 34)$ and $X|(Y = \text{No}) \sim N(10, 34)$. Furthermore, we know that $\mathbb{E}[Y] = P(Y = 1) = 0.8$. I use the following \texttt{R} code to determine $P(X| Y = \text{Yes})$ and $P(X | Y = \text{No})$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{pxNo} \hlkwb{<-} \hlkwd{dnorm}\hlstd{(}\hlnum{4}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlkwd{sqrt}\hlstd{(}\hlnum{36}\hlstd{))}
\hlstd{pxYes} \hlkwb{<-} \hlkwd{dnorm}\hlstd{(}\hlnum{4}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{10}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlkwd{sqrt}\hlstd{(}\hlnum{36}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

and now we do the full bayes theorem: $$ P(Y = 1 | X) = \frac{P(X|Y = 1) P(Y = 1)}{ P(X|Y=1)P(Y = 1) + P(X | Y = 0)P(Y = 0)}$$


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(}\hlnum{0.8} \hlopt{*} \hlstd{pxYes)}\hlopt{/}\hlstd{(}\hlnum{0.8} \hlopt{*} \hlstd{pxYes} \hlopt{+} \hlnum{0.2} \hlopt{*} \hlstd{pxNo)}
\end{alltt}
\begin{verbatim}
## [1] 0.7518525
\end{verbatim}
\end{kframe}
\end{knitrout}

So $P(Y = \text{Yes} | X = 4) = 0.752$. 


\end{enumerate}

\end{document}
